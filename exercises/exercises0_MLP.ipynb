{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. MLP for regression (*)\n",
    "We will continue with the dataset that we used in the lecture with predicting miles per gallons using an MLP for regression.\n",
    "\n",
    "a) Load the mpg dataset using seaborn. (*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sns.load_dataset(\"mpg\").drop(\"name\", axis=1)\n",
    "mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use your data analysis skills to perform EDA. (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.info() #Six missing values for horsepower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the data\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6), dpi=100)\n",
    "\n",
    "for ax, feature in zip(axes.flatten(), mpg.columns[2:6]):\n",
    "  sns.scatterplot(data=mpg, x=feature, y=\"mpg\", ax = ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "_ = fig.suptitle(\"Possible predictors for mpg\", y=1.03, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(mpg, corner=True, height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Find out the missing values in the dataset and use a machine learning model to fill them in (imputation). (**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy code origin to be able to use it as a predictor\n",
    "mpg = pd.get_dummies(mpg, columns=[\"origin\"], drop_first=True)\n",
    "mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks the rows containing missing values and remove horsepower\n",
    "rows_to_impute = mpg.query(\"horsepower.isna()\").drop(\"horsepower\", axis=1)\n",
    "rows_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values from the full dataset and split it into X and y\n",
    "mpg_dropped_missing = mpg.dropna()\n",
    "X, y = mpg_dropped_missing.drop(\"horsepower\", axis = 1), mpg_dropped_missing[\"horsepower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and impute the values\n",
    "model_lin_reg = LinearRegression()\n",
    "model_lin_reg.fit(X, y)\n",
    "rows_to_impute[\"horsepower\"] = model_lin_reg.predict(rows_to_impute)\n",
    "rows_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "mpg_imputed = pd.concat([mpg_dropped_missing, rows_to_impute]).sort_index()\n",
    "mpg_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Can you figure out a way to see if the values filled in are reasonable? (**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can for example check MAE and RMSE for how well the model predicts horsepower. As can be seen below, the model is pretty good at predicting horsepower. I therefore believe that the values filled in are reasonable. However, this assumption, is based on that the values are missing at random. \n",
    "\n",
    "Also, we should be a bit cautious when using this approach on too many values. We have now removed the noise for the imputed values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and make predictions\n",
    "model_lin_reg = LinearRegression()\n",
    "model_lin_reg.fit(X_train, y_train)\n",
    "y_pred = model_lin_reg.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"MAE: {MAE:.2f}\")\n",
    "print(f\"RMSE: {RMSE:.2f}\")\n",
    "print(f\"Horsepower range: {y_test.max() - y_test.min()}\")\n",
    "print(f\"Horsepower SD: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Do a train|val|test split on the data and scale it properly. Test out which scaling method to use. (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mpg_imputed.drop(\"mpg\", axis=1).values, mpg_imputed[\"mpg\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_stand = standard_scaler.fit_transform(X_train)\n",
    "X_val_stand = standard_scaler.transform(X_val)\n",
    "X_train_norm = min_max_scaler.fit_transform(X_train)\n",
    "X_val_norm = min_max_scaler.transform(X_val)\n",
    "\n",
    "model_lin_reg = LinearRegression()\n",
    "model_lin_reg.fit(X_train_stand, y_train)\n",
    "y_pred = model_lin_reg.predict(X_val_stand)\n",
    "MAE = mean_absolute_error(y_val, y_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(MAE, RMSE)\n",
    "\n",
    "model_lin_reg = LinearRegression()\n",
    "model_lin_reg.fit(X_train_norm, y_train)\n",
    "y_pred = model_lin_reg.predict(X_val_norm)\n",
    "MAE = mean_absolute_error(y_val, y_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(MAE, RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Create an MLP with hidden layers, 1-3, and test out different amount of nodes. Choose the number of epochs you want to use throughout all experiments. Plot training losses and validation losses for different configurations. (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_MLP(number_of_layers, number_of_nodes):\n",
    "    model_MLP = Sequential(name = \"MLP\") # Here we add the input layer\n",
    "    \n",
    "    model_MLP.add(InputLayer(X_train_stand.shape[1])) # We specify the number of features\n",
    "    \n",
    "    model_MLP.add(Dense(20, name=\"Hidden_layer_1\")) # Hidden layers\n",
    "    if number_of_layers == 2 or number_of_layers == 3:\n",
    "        model_MLP.add(Dense(number_of_nodes, name=\"Hidden_layer_2\")) # Hidden layers\n",
    "        if number_of_layers == 3:\n",
    "            model_MLP.add(Dense(number_of_nodes, name=\"Hidden_layer_3\")) # Hidden layers\n",
    "\n",
    "    model_MLP.add(Dense(1, name = \"Output_layer\")) # Note no activation function --> linear activation\n",
    "\n",
    "    model_MLP.compile(loss = \"mean_squared_error\", optimizer = SGD(learning_rate=.01))\n",
    "    \n",
    "    return model_MLP\n",
    "\n",
    "model_MLP = KerasRegressor(build_fn=model_MLP, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_MLP = Pipeline([\n",
    "                        ('scaler', None),\n",
    "                        ('model_MLP', model_MLP)\n",
    "]) \n",
    "\n",
    "param_grid_MLP = {\n",
    "                \"scaler\" : [StandardScaler(), MinMaxScaler()],\n",
    "                \"model_MLP__number_of_layers\" : [1, 2, 3],\n",
    "                \"model_MLP__number_of_nodes\" : [10, 50, 100]\n",
    "} \n",
    "\n",
    "CV_results_MLP = GridSearchCV(estimator=pipeline_MLP, param_grid=param_grid_MLP, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_results_MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_results_MLP.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP = Sequential(name = \"MLP\") # Here we add the input layer\n",
    "model_MLP.add(InputLayer(X_train_stand.shape[1])) # We specify the number of features\n",
    "model_MLP.add(Dense(20, name=\"Hidden_layer_1\")) # Hidden layers\n",
    "model_MLP.add(Dense(1, name = \"Output_layer\")) # Note no activation function --> linear activation\n",
    "model_MLP.compile(loss = \"mean_squared_error\", optimizer = SGD(learning_rate=.01))\n",
    "model_MLP.fit(X_train_stand, y_train, epochs = 50, verbose = 1, validation_data=(X_val_stand, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Now use early stopping to tune the number of epochs. (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Train on all training data and validation data. (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Predict on test data and evaluate. (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Can you create an MLP model that beats random forest for this dataset? (**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8622f42dd2024023add1dc3af682913d650e689fa358b3965b6044288c201ea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('Deep-Learning-Anna-MariaSjolund-0DMhVJHL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
